{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Install and Setup Airflow\n",
    "2. Make Directories\n",
    "3. Tensorflow Extended Module Files\n",
    "4. Migrate Baseline Data\n",
    "5. Create Airflow Pipeline File\n",
    "6. Continuous Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to walkthrough a TFX pipeline orchestrated using Airflow. Since we prototyped the pipeline in the interactive pipeline, we can reuse a lot of the components to complete the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Setup Airflow\n",
    "To install Airflow, we need to define the location of Airflow and we can install it with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export AIRFLOW_HOME=~/airflow\n",
    "pip install apache-airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Airflow is installed, we need to initialize the Airflow database. Out of the box, Airflow will work with SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "airflow initdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual Airflow setup consists of an Airflow scheduler and web server. The scheduler is responsible for coordinate the tasks while the web server provides a user interface to interact with the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Open a new terminal window\n",
    "airflow webserver -p 8080\n",
    "\n",
    "# Open in a separate terminal window\n",
    "airflow scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is working then we should be able to access Airflow by going to http://127.0.0.1:8080. You may need enter the shell commands below to setup an account to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "airflow users  create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make Directories\n",
    "To keep our project organized, we will move all associated files in the Airflow directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(os.environ['HOME'], 'airflow' 'data', 'hot_dog_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensorflow Extended Module Files\n",
    "We have defined the module files in the interactive pipeline. We can move them into the Airflow folder for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hot_dog_transform = os.path.join(\n",
    "    os.environ['HOME'], 'airflow', 'dags', 'hot_dog_transform.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_hot_dog_transform}\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _process_image(raw_image):\n",
    "    \"\"\"Process a single image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_image : bytestring\n",
    "        Encoded image string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        Decoded and resized image\n",
    "    \"\"\"\n",
    "    raw_image = tf.reshape(raw_image, [])\n",
    "    img_rgb = tf.image.decode_jpeg(raw_image, channels=3)\n",
    "    img = tf.cast(img_rgb, dtype=tf.float32)\n",
    "    resized_img = tf.image.resize_with_crop_or_pad(\n",
    "        img, target_height=224, target_width=224,\n",
    "    )\n",
    "    \n",
    "    return tf.reshape(resized_img, [224, 224, 3])\n",
    " \n",
    " \n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Callback function for preprocessing inputs\n",
    "\n",
    "    Serves as the entry point for TFX Transform component\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : nested tf.Tensor\n",
    "        A batch of tensors to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        Each tensor stacks the results of applying fn to tensors unstacked from \n",
    "        elems along the first dimension, from first to last\n",
    "    \"\"\"\n",
    "    image_raw = inputs['image']\n",
    "    label = inputs['label']\n",
    "    # the pipeline processes images in batches\n",
    "    # use the tf.map_fn to apply our user defined function to batch\n",
    "    img_preprocessed=tf.map_fn(_process_image, image_raw, dtype=tf.float32)\n",
    "\n",
    "    return {\n",
    "      'image_xf': img_preprocessed,\n",
    "      'label': label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hot_dog_train = os.path.join(\n",
    "    os.environ['HOME'], 'airflow', 'dags', 'hot_dog_train.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_hot_dog_train}\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_transform import TFTransformOutput\n",
    "\n",
    "\n",
    "_LABEL_KEY = 'label'\n",
    "_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def _input_fn(\n",
    "    file_pattern, data_accessor, tf_transform_output, batch_size\n",
    "):\n",
    "    \"\"\"Generates features and label for tuning/training\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_pattern : List[str]\n",
    "        List of paths or patterns of input tfrecord files.\n",
    "    data_accessor : tfx.components.DataAccessor\n",
    "        DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output : tft.TFTransformOutput\n",
    "        Output from Transform component\n",
    "    batch_size : int\n",
    "        representing the number of consecutive elements of returned\n",
    "        dataset to combine in a single batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    dataset = data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        tfxio.TensorFlowDatasetOptions(\n",
    "            batch_size=batch_size, label_key=_LABEL_KEY,\n",
    "            shuffle_buffer_size=1200, shuffle_seed=123\n",
    "        ),\n",
    "        tf_transform_output.transformed_metadata.schema\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _build_keras_model():\n",
    "    \"\"\"Create a Keras model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.keras.Model\n",
    "        Model to be used during training\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=(224, 224, 3), name='image_xf')\n",
    "    base_model= tf.keras.applications.EfficientNetB0(\n",
    "      include_top=False, weights='imagenet', input_tensor=inputs\n",
    "    )\n",
    "\n",
    "    # Rebuild top\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    output = tf.keras.layers.Dense(3, activation='softmax', name='label')(x)\n",
    "\n",
    "    # Compile\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "          loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "          optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "          metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"\"Returns a function that parses a serialized tf.Example\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras.Model\n",
    "        Model to be used during training\n",
    "    tf_transform_output : TFTransformOutput\n",
    "        Output from Transform component\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.function\n",
    "        serve_tf_examples_fn\n",
    "    \"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        serialized_tf_examples : tf.Example\n",
    "            Serialized tf.Example to be processed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Serving signature\n",
    "        \"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(_LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(\n",
    "            serialized_tf_examples, feature_spec\n",
    "        )\n",
    "\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "\n",
    "        outputs = model(transformed_features)\n",
    "        return {\"outputs\": outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args):\n",
    "    \"\"\"Train the model based on given args\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fn_args : _type_\n",
    "        Arguments used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_dataset = _input_fn(\n",
    "      fn_args.train_files, fn_args.data_accessor, \n",
    "      tf_transform_output, _BATCH_SIZE\n",
    "    )\n",
    "    eval_dataset = _input_fn(\n",
    "      fn_args.eval_files, fn_args.data_accessor, \n",
    "      tf_transform_output, _BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    model = _build_keras_model()\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "      monitor='val_accuracy', patience=3\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=20,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    signatures = {\n",
    "          \"serving_default\": _get_serve_tf_examples_fn(\n",
    "              model, tf_transform_output\n",
    "          ).get_concrete_function(\n",
    "              tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
    "          ),\n",
    "      }\n",
    "    model.save(\n",
    "        fn_args.serving_model_dir, save_format=\"tf\", signatures=signatures\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Migrate Baseline Data\n",
    "Similar to the interactive pipeline, we can use the baseline data set to get an initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy(\n",
    "    '../data/processed/baseline.tfrecord', \n",
    "    os.path.join(\n",
    "        os.environ['HOME'], 'airflow', 'data', 'hot_dog_data', \n",
    "        'baseline.tfrecord'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Airflow Pipeline File\n",
    "We can reuse the components of the interactive pipeline. For an Airflow pipeline, the TFX components get wrapped in a single pipeline function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hot_dog_pipeline = os.path.join(\n",
    "    os.environ['HOME'], 'airflow', 'dags', 'hot_dog_pipeline.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_hot_dog_pipeline}\n",
    "\n",
    "\"\"\"TFX pipeline orchestrated with Airflow\"\"\"\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "from typing import List\n",
    "#from tfx.proto import example_gen_pb2\n",
    "from tfx import v1 as tfx\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.airflow.airflow_dag_runner import AirflowDagRunner\n",
    "from tfx.orchestration.airflow.airflow_dag_runner import AirflowPipelineConfig\n",
    "\n",
    "\n",
    "_pipeline_name = 'hot_dog_classifier'\n",
    "_airflow_root = os.path.join(os.environ['HOME'], 'airflow')\n",
    "\n",
    "# Location where data is stored; defined earlier in this notebook\n",
    "_data_root = os.path.join(_airflow_root, 'data', 'hot_dog_data')\n",
    "\n",
    "# Location where module files are stored; defined earlier in this notebook\n",
    "_transform_module_file = os.path.join(\n",
    "    _airflow_root, 'dags', 'hot_dog_transform.py'\n",
    ")\n",
    "_trainer_module_file = os.path.join(\n",
    "    _airflow_root, 'dags', 'hot_dog_train.py'\n",
    ")\n",
    "\n",
    "# Location to output the trained model\n",
    "_serving_model_dir = os.path.join(_airflow_root, 'models', 'taco_bias')\n",
    "\n",
    "# Location to save artifacts and metadata\n",
    "_tfx_root = os.path.join(_airflow_root, 'tfx')\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines', _pipeline_name)\n",
    "# Sqlite ML-metadata db path.\n",
    "_metadata_path = os.path.join(\n",
    "    _tfx_root, 'metadata', _pipeline_name, 'metadata.db'\n",
    ")\n",
    "\n",
    "_beam_pipeline_args = [\n",
    "    '--direct_running_mode=multi_processing',\n",
    "    # setting direct_num_workers=0 will auto-detect available number of CPUs\n",
    "    '--direct_num_workers=0',\n",
    "]\n",
    "\n",
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "# we can control the frequency of continuous training using schedule_interval\n",
    "_airflow_config = {\n",
    "    'schedule_interval': '0 */2 * * *',\n",
    "    'start_date': datetime.datetime(2019, 1, 1)\n",
    "}\n",
    "\n",
    "\n",
    "def _create_pipeline(\n",
    "    pipeline_name, pipeline_root, data_root, transform_module_file, \n",
    "    trainer_module_file, serving_model_dir, metadata_path, beam_pipeline_args\n",
    "):\n",
    "    \"\"\"Create a TFX Pipeline\"\"\"\n",
    "    example_gen = tfx.components.ImportExampleGen(input_base=data_root)\n",
    "\n",
    "    statistics_gen = tfx.components.StatisticsGen(\n",
    "        examples=example_gen.outputs['examples']\n",
    "    )\n",
    "    \n",
    "    schema_gen = tfx.components.SchemaGen(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        infer_feature_shape=True\n",
    "    )\n",
    "    \n",
    "    example_validator = tfx.components.ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema']\n",
    "    )\n",
    "    \n",
    "    transform = tfx.components.Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        module_file=os.path.abspath(transform_module_file)\n",
    "    )\n",
    "  \n",
    "    # Uses user-provided Python function that implements a model.\n",
    "    trainer = tfx.components.Trainer(\n",
    "        #custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "        module_file=os.path.abspath(trainer_module_file),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=6),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=2)\n",
    "    )\n",
    "\n",
    "    model_resolver = tfx.dsl.Resolver(\n",
    "        strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "        model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "        model_blessing=tfx.dsl.Channel(\n",
    "            type=tfx.types.standard_artifacts.ModelBlessing\n",
    "        )\n",
    "    ).with_id('latest_blessed_model_resolver')\n",
    "\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='label')],\n",
    "        slicing_specs=[tfma.SlicingSpec()],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(\n",
    "                    class_name='CategoricalAccuracy',\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={'value': 0.55}),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={'value': -1e-3}\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "    evaluator = tfx.components.Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=eval_config\n",
    "    )  \n",
    "    pusher = tfx.components.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=evaluator.outputs['blessing'],\n",
    "        push_destination=tfx.proto.PushDestination(\n",
    "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[\n",
    "            example_gen,\n",
    "            statistics_gen,\n",
    "            schema_gen,\n",
    "            example_validator,\n",
    "            transform,\n",
    "            trainer,\n",
    "            model_resolver,\n",
    "            evaluator,\n",
    "            pusher,\n",
    "        ],\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path\n",
    "        ), \n",
    "        beam_pipeline_args=beam_pipeline_args\n",
    "    )\n",
    "\n",
    "# 'DAG' below need to be kept for Airflow to detect dag.\n",
    "DAG = AirflowDagRunner(AirflowPipelineConfig(_airflow_config)).run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=_pipeline_name,\n",
    "        pipeline_root=_pipeline_root,\n",
    "        data_root=_data_root,\n",
    "        transform_module_file=_transform_module_file,\n",
    "        trainer_module_file=_trainer_module_file,\n",
    "        serving_model_dir=_serving_model_dir,\n",
    "        metadata_path=_metadata_path,\n",
    "        beam_pipeline_args=_beam_pipeline_args\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Continuous Training\n",
    "The Airflow model is scheduled to rerun at regular intervals. We can simulate continuous training by adding the taco bias data to folder with the baseline data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy(\n",
    "    '../data/processed/taco_bias.tfrecord', \n",
    "    os.path.join(\n",
    "        os.environ['HOME'], 'airflow', 'data', 'hot_dog_data', \n",
    "        'taco_bias.tfrecord'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, an Airflow DAG can be set up to do this process automatically. The `sensor_task` watches to see if any file is created in the path `~/airflow/models/taco_bias`. Once a new file is created, `python_task` uses `shutil.copy` to move the tfrecord file from `~/Documents/GitHub/hot_dog_classifier/data/processed/taco_bias.tfrecord` to `~/airflow/data/hot_dog_data` in anticipation of the next run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_move_file = os.path.join(\n",
    "    os.environ['HOME'], 'airflow', 'dags', 'move_file.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_move_file}\n",
    "\n",
    "\"\"\"Transfer TFRecord once model is completed\"\"\"\n",
    "\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "import datetime\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "import airflow\n",
    "import shutil\n",
    "import os\n",
    "default_args = {\n",
    "    'start_date': datetime.datetime(2019, 1, 1)\n",
    "}\n",
    "\n",
    "with airflow.DAG(\n",
    "    'move_file', default_args= default_args, schedule_interval= '@once'\n",
    ") as dag:\n",
    "    sensor_task = FileSensor(\n",
    "        task_id= 'file_sensor_task', poke_interval=30,  filepath=os.path.join(\n",
    "                os.environ['HOME'], 'airflow', 'models', 'taco_bias', '*'\n",
    "            )\n",
    "        )\n",
    "    python_task = PythonOperator(\n",
    "        task_id='move_file',\n",
    "        python_callable=shutil.copy,\n",
    "        op_kwargs={\n",
    "            'src': os.path.join(\n",
    "                os.environ['HOME'], 'Documents', 'GitHub', \n",
    "                'hot_dog_classifier', 'data', 'processed', 'taco_bias.tfrecord'\n",
    "            ), \n",
    "            'dst': os.path.join(\n",
    "                os.environ['HOME'], 'airflow', 'data', 'hot_dog_data'\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "sensor_task >> python_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is retrained, we can save it as part of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "copy_tree(\n",
    "    os.path.join(os.environ['HOME'], 'airflow', 'models', 'taco_bias'), \n",
    "    '../data/models/taco_bias'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c269fd0968aad84b50af17ff3b32b119e1bf985e9dc11f0ec3b494b3c725c1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
