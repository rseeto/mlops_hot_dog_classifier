{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Setup Google Cloud Project\n",
    "2. Setup Jupyter Notebook\n",
    "3. Tensorflow Extended Module Files\n",
    "4. Migrate Baseline Data\n",
    "5. Write a Vertex AI/Kubeflow Definition File\n",
    "6. Submit Pipeline\n",
    "7. Continuous Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to walkthrough a TFX pipeline orchestrated using a managed version of Kubeflow Pipelines on Google Cloud Platform (Vertex Pipelines). Like the Airflow example, the core TFX components can be reused with minor modifications and the pipeline will be initiated through a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Google Cloud project\n",
    "\n",
    "There are some things we need to do before starting our pipeline on Google Cloud Platform. \n",
    "\n",
    "It is necessary to [create a Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects) which allow us to keep everything related to the pipeline together.\n",
    "\n",
    "Once a project is created, we need to [enable the appropriate APIs](https://support.google.com/googleapi/answer/6158841?hl=en&ref_topic=7013279). Enabling the appropriate APIs allows us to access the services necessary for this project. We will be running a pipeline orchestrated using a managed Kubeflow Pipelines instance; therefore, we will need to enable the Vertex AI, Compute Engine, and Notebooks API. In addition, we will need to enable the Cloud Functions, Cloud Build, and Cloud Resource Manager API to create a Cloud Functions function used for continuous training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can enter commands into Google Cloud Platform's `Cloud Shell`, an online bash shell based on Debian, to help us manage our project. We can [lauch the `Cloud Shell` from the Google Cloud console](https://cloud.google.com/shell/docs/launching-cloud-shell).\n",
    "\n",
    "It is necessary to create a service to run our pipeline. With the appropriate permissions, a service account can make authorized API calls and interact with the enabled APIs. We can enter the following code into the `Cloud Shell` to create a service account.\n",
    "```\n",
    "gcloud iam service-accounts create SERVICE_ACCOUNT_ID \\\n",
    "    --display-name=\"DISPLAY_NAME\" \\\n",
    "    --project=PROJECT_ID\n",
    "```\n",
    "With the following definitions:\n",
    "- `SERVICE_ACCOUNT_ID`: ID of the created service account\n",
    "- `DISPLAY_NAME`: display name of the service account\n",
    "- `PROJECT_ID`: project the service account is associated with\n",
    "\n",
    "In the case of this project, the following was entered into the `Cloud Shell`:\n",
    "```\n",
    "gcloud iam service-accounts create hot-dog-pipeline-service\\\n",
    "    --display-name=\"hot-dog-pipeline-service\" \\\n",
    "    --project=\"hot-dog-pipeline\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a service account is created, it is necessary to give the service account a role. On Google Cloud Platform, a role is associated with the necessary permissions to interact with the API. This role should include all the necessary permissions to successfully run the pipeline.\n",
    "\n",
    "```\n",
    "gcloud projects add-iam-policy-binding PROJECT_ID\\\n",
    "    --member=\"serviceAccount:SERVICE_ACCOUNT_ID@PROJECT_ID.iam.gserviceaccount.com\" \\\n",
    "    --role=\"ROLE\"\n",
    "```\n",
    "With the following definitions:\n",
    "- `SERVICE_ACCOUNT_ID`: ID of the created service account\n",
    "- `PROJECT_ID`: project the service account is associated with\n",
    "- `ROLE`: role to associate with the service account\n",
    "\n",
    "Since we will be interacting with Vertex AI, we need to give the service account an appropriate role and associated permissions. In the case of this project, the following was entered into the `Cloud Shell`:\n",
    "```\n",
    "gcloud projects add-iam-policy-binding hot-dog-pipeline \\\n",
    "    --member=\"serviceAccount:hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com\" \\\n",
    "    --role=\"roles/aiplatform.serviceAgent\"\n",
    "```\n",
    "\n",
    "We also want to grant the service accout the Cloud Function Developer. This will be helpful for continuous training.\n",
    "\n",
    "```\n",
    "gcloud projects add-iam-policy-binding hot-dog-pipeline \\\n",
    "    --member=\"serviceAccount:hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com\" \\\n",
    "    --role=\"roles/cloudfunctions.developer\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a service account set up with the appropriate roles and permissions, it is necessary to associate the service account with the user account so the user can run the pipeline.\n",
    "\n",
    "```\n",
    "gcloud iam service-accounts add-iam-policy-binding \\\n",
    "    SERVICE_ACCOUNT_ID@PROJECT_ID.iam.gserviceaccount.com \\\n",
    "    --member=\"user:USER_EMAIL\" \\\n",
    "    --role=\"roles/iam.serviceAccountUser\"\n",
    "```\n",
    "With the following definitions:\n",
    "- `SERVICE_ACCOUNT_ID`: ID of the created service account\n",
    "- `PROJECT_ID`: project the service account is associated with\n",
    "- `USER_EMAIL`: email address of user who will run pipeline\n",
    "\n",
    "In the case of this project, the following was entered into the `Cloud Shell` (the user email has been removed for privacy issues):\n",
    "\n",
    "```\n",
    "gcloud iam service-accounts add-iam-policy-binding \\\n",
    "    hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com \\\n",
    "    --member=\"user:@gmail.com\" \\\n",
    "    --role=\"roles/iam.serviceAccountUser\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a storage account to hold all the files associated with this project.\n",
    "\n",
    "```\n",
    "gsutil mb -p PROJECT_ID -l LOCATION gs://BUCKET_NAME\n",
    "```\n",
    "With the following definitions:\n",
    "- `PROJECT_ID`: project the service account is associated with\n",
    "- `LOCATION`: Google Cloud Platform zone we want the bucket located in\n",
    "- `BUCKET_NAME`: what we want to call the bucket\n",
    "\n",
    "Note: Bucket names should be [globally unique](https://cloud.google.com/storage/docs/naming-buckets); therefore, a new bucket name is necessary if you are following alone with this notebook.\n",
    "\n",
    "The following was entered into the `Cloud Shell`:\n",
    "```\n",
    "gsutil mb -p hot-dog-pipeline -l US-CENTRAL1 gs://hot-dog-pipeline-gcs\n",
    "```\n",
    "\n",
    "We also need a separate bucket we used exclusively for data. This is because the Cloud Functions does not take a subdirectory as a `trigger-resource` (see '7. Continuous Training').\n",
    "```\n",
    "gsutil mb -p hot-dog-pipeline -l US-CENTRAL1 gs://hot-dog-pipeline-gcs-data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grant the service account storage privledges so there is read and write access in the storage buckets for the pipeline artifacts.\n",
    "\n",
    "```\n",
    "gsutil iam ch \\\n",
    "serviceAccount:hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com:roles/storage.admin \\\n",
    "gs://hot-dog-pipeline-gcs\n",
    "\n",
    "gsutil iam ch \\\n",
    "serviceAccount:hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com:roles/storage.admin \\\n",
    "gs://hot-dog-pipeline-gcs-data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create a Jupyter notebook to run the rest of this notebook in. We are using a [Deep Learning VM Image](https://cloud.google.com/deep-learning-vm), specifically tf2-ent-2-7-cu113-notebooks-v20211202-debian-10, as the image contains preinstalled key machine learning frameworks and tools. The options of the `gcloud notebooks instances create` are too long but are available [here](https://cloud.google.com/sdk/gcloud/reference/notebooks/instances/create). The following was entered into `Cloud Shell`:\n",
    "```\n",
    "gcloud notebooks instances create hot-dog-pipeline-notebook \\\n",
    "    --vm-image-project=\"deeplearning-platform-release\" \\\n",
    "    --vm-image-name=tf2-ent-2-7-cu113-notebooks-v20211202-debian-10 \\\n",
    "    --machine-type=n1-standard-2 \\\n",
    "    --location=us-central1-a \\\n",
    "    --service-account=hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VuwrlnvQJ5k"
   },
   "source": [
    "Once the notebook is created, it can be [opened](https://cloud.google.com/vertex-ai/docs/workbench/user-managed/create-user-managed-notebooks-instance-console-quickstart#open-jupyterlab). A copy of this notebook can be uploaded and opened in the JupyterLab instance. The rest of this notebook can be run in the JupyterLab instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup notebook\n",
    "\n",
    "Even though the Deep Learning VM contains machine learning frameworks and tools, we should still use the latest versions. Therefore, it is necessary to upgrade pip and the libraries of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iyQtljP-qPHY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (21.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 4.8 MB/s            \n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.3.1\n",
      "    Uninstalling pip-21.3.1:\n",
      "      Successfully uninstalled pip-21.3.1\n",
      "Successfully installed pip-22.2.2\n",
      "Collecting tfx[kfp]<2\n",
      "  Downloading tfx-1.9.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting docker<5,>=4.1\n",
      "  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.10.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting tfx-bsl<1.10.0,>=1.9.0\n",
      "  Downloading tfx_bsl-1.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform<2,>=1.6.2 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (1.7.1)\n",
      "Collecting click<8,>=7\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (3.19.1)\n",
      "Collecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5\n",
      "  Downloading tensorflow-2.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.38\n",
      "  Downloading apache_beam-2.41.0-cp37-cp37m-manylinux2010_x86_64.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (1.19.5)\n",
      "Collecting google-api-core<2\n",
      "  Downloading google_api_core-1.33.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-transform<1.10.0,>=1.9.0\n",
      "  Downloading tensorflow_transform-1.9.0-py3-none-any.whl (436 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3,>=2.26.0 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (2.30.1)\n",
      "Collecting ml-pipelines-sdk==1.9.1\n",
      "  Downloading ml_pipelines_sdk-1.9.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portpicker<2,>=1.3.1\n",
      "  Downloading portpicker-1.5.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: grpcio<2,>=1.28.1 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (1.42.0)\n",
      "Collecting pyarrow<6,>=1\n",
      "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-model-analysis<0.41,>=0.40.0\n",
      "  Downloading tensorflow_model_analysis-0.40.0-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub<0.13,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (0.12.0)\n",
      "Requirement already satisfied: jinja2<4,>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (2.11.3)\n",
      "Requirement already satisfied: keras-tuner<2,>=1.0.4 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.10.0.2 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (4.0.1)\n",
      "Requirement already satisfied: google-apitools<1,>=0.5 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (0.5.31)\n",
      "Collecting ml-metadata<1.10.0,>=1.9.0\n",
      "  Downloading ml_metadata-1.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting packaging<21,>=20\n",
      "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml<6,>=3.12\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.6/636.6 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-data-validation<1.10.0,>=1.9.0\n",
      "  Downloading tensorflow_data_validation-1.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs<21,>=19.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kubernetes<13,>=10.0.1\n",
      "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py<2.0.0,>=0.9 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]<2) (1.0.0)\n",
      "Collecting google-api-python-client<2,>=1.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kfp-pipeline-spec<0.2,>=0.1.10\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting kfp<2,>=1.8.5\n",
      "  Downloading kfp-1.8.13.tar.gz (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.1/300.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2.0.0,>=0.9->tfx[kfp]<2) (1.16.0)\n",
      "Collecting cloudpickle<3,>=2.1.0\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (3.6.4)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (3.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (2.26.0)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (1.19.8)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (2.8.2)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (1.7)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (2021.3)\n",
      "Requirement already satisfied: httplib2<0.21.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.20.2)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (2.6.0)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (1.4.7)\n",
      "Collecting google-cloud-pubsub<3,>=2.1.0\n",
      "  Downloading google_cloud_pubsub-2.13.6-py2.py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.1/235.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-bigquery-storage<2.14,>=2.6.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (2.10.1)\n",
      "Collecting google-cloud-datastore<2,>=1.8.0\n",
      "  Downloading google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.2/134.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (2.3.3)\n",
      "Collecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0\n",
      "  Downloading google_cloud_pubsublite-1.4.3-py2.py3-none-any.whl (267 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.2/267.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0\n",
      "  Downloading google_cloud_dlp-3.9.0-py2.py3-none-any.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0\n",
      "  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.1.0)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-core<3,>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (2.2.1)\n",
      "Collecting google-cloud-vision<2,>=0.38.0\n",
      "  Downloading google_cloud_vision-1.0.2-py2.py3-none-any.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.1/435.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-recommendations-ai<0.8.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.2.0)\n",
      "Collecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Downloading google_cloud_bigtable-1.7.2-py2.py3-none-any.whl (267 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0\n",
      "  Downloading google_cloud_spanner-1.19.3-py2.py3-none-any.whl (255 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker<5,>=4.1->tfx[kfp]<2) (1.2.1)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.7/211.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<4,>=3.13\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.8->tfx[kfp]<2) (3.0.1)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx[kfp]<2) (0.16.3)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx[kfp]<2) (4.1.3)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx[kfp]<2) (1.43.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx[kfp]<2) (2.2.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=2.26.0->tfx[kfp]<2) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2<4,>=2.7.3->tfx[kfp]<2) (1.1.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx[kfp]<2) (1.7.3)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx[kfp]<2) (7.30.0)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx[kfp]<2) (2.7.0)\n",
      "Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx[kfp]<2) (1.0.4)\n",
      "Collecting google-auth<3,>=1.18.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp<2,>=1.8.5->tfx[kfp]<2) (1.8.2)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.6.1-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=10.0.1->tfx[kfp]<2) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=10.0.1->tfx[kfp]<2) (1.3.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=10.0.1->tfx[kfp]<2) (1.26.7)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=10.0.1->tfx[kfp]<2) (59.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<21,>=20->tfx[kfp]<2) (3.0.6)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from portpicker<2,>=1.3.1->tfx[kfp]<2) (5.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tfx[kfp]<2) (1.1.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tfx[kfp]<2) (0.4.0)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting numpy<2,>=1.16\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tfx[kfp]<2) (1.13.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tfx[kfp]<2) (1.6.3)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tfx[kfp]<2) (1.1.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tfx[kfp]<2) (3.6.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tfx[kfp]<2) (3.3.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5\n",
      "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m159.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m00:01\u001b[0mCollecting google-api-core[grpc]<3.0.0dev,>=1.26.0\n",
      "  Downloading google_api_core-2.10.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.9.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.8.1-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.8.0-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.7.3-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.7.2-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.7.1-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.7.0-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.6.1-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.6.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.5.0-py2.py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.8/111.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.4.0-py2.py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.3.2-py2.py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.3.0-py2.py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.2.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.2.0-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.1.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.1.0-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.0.1-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.0.0-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.4/92.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2->tfx[kfp]<2) (1.42.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.2.7)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage<2.14,>=2.6.3->apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.3.23)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.12.3)\n",
      "Collecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting overrides<7.0.0,>=6.0.1\n",
      "  Downloading overrides-6.2.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=2.26.0->tfx[kfp]<2) (1.1.2)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tfx[kfp]<2) (1.5.2)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.6.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (0.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (3.0.22)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (5.1.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (2.10.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (0.7.5)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (1.0.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (5.1.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (3.5.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (6.5.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp<2,>=1.8.5->tfx[kfp]<2) (4.8.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp<2,>=1.8.5->tfx[kfp]<2) (0.18.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx[kfp]<2) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (3.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (2.0.2)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (1.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx[kfp]<2) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=2.26.0->tfx[kfp]<2) (1.15.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (1.5.1)\n",
      "Requirement already satisfied: argcomplete>=1.12.3 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (1.12.3)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (7.1.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp<2,>=1.8.5->tfx[kfp]<2) (3.6.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (0.8.3)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage<2.14,>=2.6.3->apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.7.1)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (4.9.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner<2,>=1.0.4->tfx[kfp]<2) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (6.4.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=2.26.0->tfx[kfp]<2) (2.21)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (1.5.4)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (6.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (21.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.12.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (1.8.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage<2.14,>=2.6.3->apache-beam[gcp]<3,>=2.38->tfx[kfp]<2) (0.4.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.7.1)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.5.9)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.8.4)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.41,>=0.40.0->tfx[kfp]<2) (0.5.1)\n",
      "Building wheels for collected packages: kfp, dill, fire, kfp-server-api, pyfarmhash, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.8.13-py3-none-any.whl size=422444 sha256=c5d0295030f7f91bc483f300d8a3e30cf303b96061761fa98764f525fda40788\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/49/00/38/b7f496476d9160ea493829999bb52ad13dcafaa5d1bbb6a09a\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=3195d3dc1cf5ce9980516a1fae5b4ec9a40f6676fe08ed233604ffc8b4ff67cc\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=5b72823382d4bda5ebb6d1d0a77f84cb5a621b5f41080d1859259dee5ff5bd0f\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99715 sha256=d7c4be943d4b4171b76fc26205b8b865b834e8a7ea8db36bab97652bfbbdbe9a\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/77/0e/7b/ed385d69453b7b754834c01d83fa9f5708ba66b4f6ed5d6a35\n",
      "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp37-cp37m-linux_x86_64.whl size=108633 sha256=49bc3b403188ac6400ba39045ee45c5f5b44399d8fb4b29b86217c7eef81d73c\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/53/58/7a/3b040f3a2ee31908e3be916e32660db6db53621ce6eba838dc\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=d9bb88c72554bd333f90d92c82115ed4f433761285d9de4529034318c3fb7830\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp dill fire kfp-server-api pyfarmhash strip-hints\n",
      "Installing collected packages: pyfarmhash, libclang, keras, joblib, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, tabulate, strip-hints, pyyaml, protobuf, portpicker, packaging, overrides, numpy, fire, docstring-parser, dill, Deprecated, cloudpickle, click, attrs, typer, requests-toolbelt, pyarrow, proto-plus, ml-metadata, kfp-server-api, kfp-pipeline-spec, jsonschema, googleapis-common-protos, google-auth, docker, tensorflow-metadata, kubernetes, google-api-core, apache-beam, tensorboard, grpc-google-iam-v1, google-api-python-client, tensorflow, ml-pipelines-sdk, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, tensorflow-serving-api, kfp, google-cloud-pubsublite, tfx-bsl, tensorflow-transform, tensorflow-model-analysis, tensorflow-data-validation, tfx\n",
      "  Attempting uninstall: libclang\n",
      "    Found existing installation: libclang 12.0.0\n",
      "    Uninstalling libclang-12.0.0:\n",
      "      Successfully uninstalled libclang-12.0.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.7.0\n",
      "    Uninstalling keras-2.7.0:\n",
      "      Successfully uninstalled keras-2.7.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.22.0\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.22.0:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.22.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.0.0\n",
      "    Uninstalling cloudpickle-2.0.0:\n",
      "      Successfully uninstalled cloudpickle-2.0.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.3\n",
      "    Uninstalling click-8.0.3:\n",
      "      Successfully uninstalled click-8.0.3\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 6.0.1\n",
      "    Uninstalling pyarrow-6.0.1:\n",
      "      Successfully uninstalled pyarrow-6.0.1\n",
      "  Attempting uninstall: proto-plus\n",
      "    Found existing installation: proto-plus 1.19.8\n",
      "    Uninstalling proto-plus-1.19.8:\n",
      "      Successfully uninstalled proto-plus-1.19.8\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.2.1\n",
      "    Uninstalling jsonschema-4.2.1:\n",
      "      Successfully uninstalled jsonschema-4.2.1\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.53.0\n",
      "    Uninstalling googleapis-common-protos-1.53.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.53.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.3.3\n",
      "    Uninstalling google-auth-2.3.3:\n",
      "      Successfully uninstalled google-auth-2.3.3\n",
      "  Attempting uninstall: docker\n",
      "    Found existing installation: docker 5.0.3\n",
      "    Uninstalling docker-5.0.3:\n",
      "      Successfully uninstalled docker-5.0.3\n",
      "  Attempting uninstall: tensorflow-metadata\n",
      "    Found existing installation: tensorflow-metadata 1.4.0\n",
      "    Uninstalling tensorflow-metadata-1.4.0:\n",
      "      Successfully uninstalled tensorflow-metadata-1.4.0\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 20.13.0\n",
      "    Uninstalling kubernetes-20.13.0:\n",
      "      Successfully uninstalled kubernetes-20.13.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.2.2\n",
      "    Uninstalling google-api-core-2.2.2:\n",
      "      Successfully uninstalled google-api-core-2.2.2\n",
      "  Attempting uninstall: apache-beam\n",
      "    Found existing installation: apache-beam 2.34.0\n",
      "    Uninstalling apache-beam-2.34.0:\n",
      "      Successfully uninstalled apache-beam-2.34.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "  Attempting uninstall: grpc-google-iam-v1\n",
      "    Found existing installation: grpc-google-iam-v1 0.12.3\n",
      "    Uninstalling grpc-google-iam-v1-0.12.3:\n",
      "      Successfully uninstalled grpc-google-iam-v1-0.12.3\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.31.0\n",
      "    Uninstalling google-api-python-client-2.31.0:\n",
      "      Successfully uninstalled google-api-python-client-2.31.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n",
      "  Attempting uninstall: google-cloud-vision\n",
      "    Found existing installation: google-cloud-vision 2.6.2\n",
      "    Uninstalling google-cloud-vision-2.6.2:\n",
      "      Successfully uninstalled google-cloud-vision-2.6.2\n",
      "  Attempting uninstall: google-cloud-videointelligence\n",
      "    Found existing installation: google-cloud-videointelligence 2.5.1\n",
      "    Uninstalling google-cloud-videointelligence-2.5.1:\n",
      "      Successfully uninstalled google-cloud-videointelligence-2.5.1\n",
      "  Attempting uninstall: google-cloud-spanner\n",
      "    Found existing installation: google-cloud-spanner 3.11.1\n",
      "    Uninstalling google-cloud-spanner-3.11.1:\n",
      "      Successfully uninstalled google-cloud-spanner-3.11.1\n",
      "  Attempting uninstall: google-cloud-pubsub\n",
      "    Found existing installation: google-cloud-pubsub 1.7.0\n",
      "    Uninstalling google-cloud-pubsub-1.7.0:\n",
      "      Successfully uninstalled google-cloud-pubsub-1.7.0\n",
      "  Attempting uninstall: google-cloud-language\n",
      "    Found existing installation: google-cloud-language 2.3.1\n",
      "    Uninstalling google-cloud-language-2.3.1:\n",
      "      Successfully uninstalled google-cloud-language-2.3.1\n",
      "  Attempting uninstall: google-cloud-dlp\n",
      "    Found existing installation: google-cloud-dlp 1.0.0\n",
      "    Uninstalling google-cloud-dlp-1.0.0:\n",
      "      Successfully uninstalled google-cloud-dlp-1.0.0\n",
      "  Attempting uninstall: google-cloud-datastore\n",
      "    Found existing installation: google-cloud-datastore 2.4.0\n",
      "    Uninstalling google-cloud-datastore-2.4.0:\n",
      "      Successfully uninstalled google-cloud-datastore-2.4.0\n",
      "  Attempting uninstall: google-cloud-bigtable\n",
      "    Found existing installation: google-cloud-bigtable 2.4.0\n",
      "    Uninstalling google-cloud-bigtable-2.4.0:\n",
      "      Successfully uninstalled google-cloud-bigtable-2.4.0\n",
      "  Attempting uninstall: tensorflow-serving-api\n",
      "    Found existing installation: tensorflow-serving-api 2.6.0\n",
      "    Uninstalling tensorflow-serving-api-2.6.0:\n",
      "      Successfully uninstalled tensorflow-serving-api-2.6.0\n",
      "  Attempting uninstall: tfx-bsl\n",
      "    Found existing installation: tfx-bsl 1.4.0\n",
      "    Uninstalling tfx-bsl-1.4.0:\n",
      "      Successfully uninstalled tfx-bsl-1.4.0\n",
      "  Attempting uninstall: tensorflow-transform\n",
      "    Found existing installation: tensorflow-transform 1.4.0\n",
      "    Uninstalling tensorflow-transform-1.4.0:\n",
      "      Successfully uninstalled tensorflow-transform-1.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.9.0 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.27.0 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.6 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.11 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 apache-beam-2.41.0 attrs-20.3.0 click-7.1.2 cloudpickle-2.2.0 dill-0.3.1.1 docker-4.4.4 docstring-parser-0.15 fire-0.4.0 flatbuffers-1.12 google-api-core-1.33.0 google-api-python-client-1.12.11 google-auth-1.35.0 google-cloud-bigtable-1.7.2 google-cloud-datastore-1.15.5 google-cloud-dlp-3.9.0 google-cloud-language-1.3.2 google-cloud-pubsub-2.13.6 google-cloud-pubsublite-1.4.3 google-cloud-spanner-1.19.3 google-cloud-videointelligence-1.16.3 google-cloud-vision-1.0.2 googleapis-common-protos-1.56.4 grpc-google-iam-v1-0.12.4 joblib-0.14.1 jsonschema-3.2.0 keras-2.9.0 kfp-1.8.13 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-12.0.1 libclang-14.0.6 ml-metadata-1.9.0 ml-pipelines-sdk-1.9.1 numpy-1.21.6 overrides-6.2.0 packaging-20.9 portpicker-1.5.2 proto-plus-1.22.1 protobuf-3.20.1 pyarrow-5.0.0 pyfarmhash-0.3.2 pyyaml-5.4.1 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.10 tensorboard-2.9.0 tensorflow-2.9.0 tensorflow-data-validation-1.9.0 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.27.0 tensorflow-metadata-1.9.0 tensorflow-model-analysis-0.40.0 tensorflow-serving-api-2.9.0 tensorflow-transform-1.9.0 tfx-1.9.1 tfx-bsl-1.9.0 typer-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade \"tfx[kfp]<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwT0nov5QO1M"
   },
   "source": [
    "It is necessary to restart the runtime after installing the libraries. Restarting the runtime can be accomplished by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KHTSzMygoBF6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "  \n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_SveIKxaENu"
   },
   "source": [
    "Since we've downloaded our libraries and restarted the runtime, we can now import the relevant updated libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xd-iP9wEaENu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tfx import v1 as tfx\n",
    "import kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDtLdSkvqPHe"
   },
   "source": [
    "We can setup some variables which will be used throughout the rest of this notebook. The variable definitions are as follows:\n",
    "- `GOOGLE_CLOUD_PROJECT`: Google Cloud Platform project name\n",
    "- `GOOGLE_CLOUD_REGION`: Google Cloud Platform zone we want\n",
    "- `GCS_BUCKET_NAME`: name of bucket that holds everything except the model data\n",
    "- `GCS_DATA_BUCKET_NAME`: name of bucket that holds the model data\n",
    "- `PIPELINE_NAME`: name of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CPN6UL5CazNy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'hot-dog-pipeline'\n",
    "GOOGLE_CLOUD_REGION = 'us-central1'     \n",
    "GCS_BUCKET_NAME = 'hot-dog-pipeline-gcs'\n",
    "GCS_DATA_BUCKET_NAME = 'hot-dog-pipeline-gcs-data'\n",
    "PIPELINE_NAME = 'hot-dog-vertex-pipelines'\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME\n",
    ")\n",
    "\n",
    "# Paths for users defined TFX modules.\n",
    "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME\n",
    ")\n",
    "\n",
    "# Path for data.\n",
    "DATA_ROOT = 'gs://{}'.format(GCS_DATA_BUCKET_NAME)\n",
    "\n",
    "# Path where model will be pushed\n",
    "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensorflow Extended Module Files\n",
    "We have previously defined the module files in the interactive and Airflow pipeline. We create local copies of the TFX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOT_DOG_TRANSFORM = 'hot_dog_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hot_dog_transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {HOT_DOG_TRANSFORM}\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _process_image(raw_image):\n",
    "    \"\"\"Process a single image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_image : bytestring\n",
    "        Encoded image string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        Decoded and resized image\n",
    "    \"\"\"\n",
    "    raw_image = tf.reshape(raw_image, [])\n",
    "    img_rgb = tf.image.decode_jpeg(raw_image, channels=3)\n",
    "    img = tf.cast(img_rgb, dtype=tf.float32)\n",
    "    resized_img = tf.image.resize_with_crop_or_pad(\n",
    "        img, target_height=224, target_width=224,\n",
    "    )\n",
    "    \n",
    "    return tf.reshape(resized_img, [224, 224, 3])\n",
    " \n",
    " \n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Callback function for preprocessing inputs\n",
    "\n",
    "    Serves as the entry point for TFX Transform component\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : nested tf.Tensor\n",
    "        A batch of tensors to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        Each tensor stacks the results of applying fn to tensors unstacked from \n",
    "        elems along the first dimension, from first to last\n",
    "    \"\"\"\n",
    "    image_raw = inputs['image']\n",
    "    label = inputs['label']\n",
    "    # the pipeline processes images in batches\n",
    "    # use the tf.map_fn to apply our user defined function to batch\n",
    "    img_preprocessed=tf.map_fn(_process_image, image_raw, dtype=tf.float32)\n",
    "\n",
    "    return {\n",
    "      'image_xf': img_preprocessed,\n",
    "      'label': label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Python file is written locally, we want to move it to our bucket so it can be accessed when the notebook instance is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://hot_dog_transform.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  1.3 KiB/  1.3 KiB]                                                \n",
      "Operation completed over 1 objects/1.3 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {HOT_DOG_TRANSFORM} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a similar process for the `hot_dog_train.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOT_DOG_TRAIN = 'hot_dog_train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hot_dog_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {HOT_DOG_TRAIN}\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_transform import TFTransformOutput\n",
    "\n",
    "\n",
    "_LABEL_KEY = 'label'\n",
    "_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def _input_fn(\n",
    "    file_pattern, data_accessor, tf_transform_output, batch_size\n",
    "):\n",
    "    \"\"\"Generates features and label for tuning/training\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_pattern : List[str]\n",
    "        List of paths or patterns of input tfrecord files.\n",
    "    data_accessor : tfx.components.DataAccessor\n",
    "        DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output : tft.TFTransformOutput\n",
    "        Output from Transform component\n",
    "    batch_size : int\n",
    "        representing the number of consecutive elements of returned\n",
    "        dataset to combine in a single batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    dataset = data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        tfxio.TensorFlowDatasetOptions(\n",
    "            batch_size=batch_size, label_key=_LABEL_KEY,\n",
    "            shuffle_buffer_size=1200, shuffle_seed=123\n",
    "        ),\n",
    "        tf_transform_output.transformed_metadata.schema\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _build_keras_model():\n",
    "    \"\"\"Create a Keras model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.keras.Model\n",
    "        Model to be used during training\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=(224, 224, 3), name='image_xf')\n",
    "    base_model= tf.keras.applications.EfficientNetB0(\n",
    "      include_top=False, weights='imagenet', input_tensor=inputs\n",
    "    )\n",
    "\n",
    "    # Rebuild top\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    output = tf.keras.layers.Dense(3, activation='softmax', name='label')(x)\n",
    "\n",
    "    # Compile\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "          loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "          optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "          metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"\"Returns a function that parses a serialized tf.Example\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras.Model\n",
    "        Model to be used during training\n",
    "    tf_transform_output : TFTransformOutput\n",
    "        Output from Transform component\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.function\n",
    "        serve_tf_examples_fn\n",
    "    \"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        serialized_tf_examples : tf.Example\n",
    "            Serialized tf.Example to be processed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Serving signature\n",
    "        \"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(_LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(\n",
    "            serialized_tf_examples, feature_spec\n",
    "        )\n",
    "\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "\n",
    "        outputs = model(transformed_features)\n",
    "        return {\"outputs\": outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args):\n",
    "    \"\"\"Train the model based on given args\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fn_args : _type_\n",
    "        Arguments used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_dataset = _input_fn(\n",
    "      fn_args.train_files, fn_args.data_accessor, \n",
    "      tf_transform_output, _BATCH_SIZE\n",
    "    )\n",
    "    eval_dataset = _input_fn(\n",
    "      fn_args.eval_files, fn_args.data_accessor, \n",
    "      tf_transform_output, _BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    model = _build_keras_model()\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "      monitor='val_accuracy', patience=3\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=20,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    signatures = {\n",
    "          \"serving_default\": _get_serve_tf_examples_fn(\n",
    "              model, tf_transform_output\n",
    "          ).get_concrete_function(\n",
    "              tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
    "          ),\n",
    "      }\n",
    "    model.save(\n",
    "        fn_args.serving_model_dir, save_format=\"tf\", signatures=signatures\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://hot_dog_train.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  4.5 KiB/  4.5 KiB]                                                \n",
      "Operation completed over 1 objects/4.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {HOT_DOG_TRAIN} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Migrate Baseline Data\n",
    "Similar to the interactive pipeline, we can use the baseline data set to get an initial model. The `baseline.tfrecord` file was [uploaded to using the console](https://cloud.google.com/storage/docs/uploading-objects#uploading-an-object). If the data set had been pushed to the appropriate github project beforehand, the command line could be used to transfer the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write a Vertex AI/Kubeflow Pipelines Definition File\n",
    "This file is very similar to Airflow's pipeline orchestration with a few exceptions. This is a pipeline orchestrated using a Kubeflow Pipelines; therefore, there are changes in the DAG runner code, and the `beam_pipeline_args` are no longer needed. Since this is a managed Kubeflow Pipelines, we no longer need the `metadata_connection_config` parameter since the metadata will be handled by Vertex Pipelines.\n",
    "\n",
    "Running the following code will create a the pipeline definition in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying hot_dog_transform.py -> build/lib\n",
      "installing to /tmp/tmp2qiq8mfu\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/hot_dog_transform.py -> /tmp/tmp2qiq8mfu\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /tmp/tmp2qiq8mfu/tfx_user_code_Transform-0.0+10dab3ab3582f483fef29f7a55750a255d7a9015b4b91ab64fb0453901d7a263-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmp2qiq8mfu/tfx_user_code_Transform-0.0+10dab3ab3582f483fef29f7a55750a255d7a9015b4b91ab64fb0453901d7a263.dist-info/WHEEL\n",
      "creating '/tmp/tmp_w1dub5b/tfx_user_code_Transform-0.0+10dab3ab3582f483fef29f7a55750a255d7a9015b4b91ab64fb0453901d7a263-py3-none-any.whl' and adding '/tmp/tmp2qiq8mfu' to it\n",
      "adding 'hot_dog_transform.py'\n",
      "adding 'tfx_user_code_Transform-0.0+10dab3ab3582f483fef29f7a55750a255d7a9015b4b91ab64fb0453901d7a263.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+10dab3ab3582f483fef29f7a55750a255d7a9015b4b91ab64fb0453901d7a263.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+10dab3ab3582f483fef29f7a55750a255d7a9015b4b91ab64fb0453901d7a263.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+10dab3ab3582f483fef29f7a55750a255d7a9015b4b91ab64fb0453901d7a263.dist-info/RECORD'\n",
      "removing /tmp/tmp2qiq8mfu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying hot_dog_train.py -> build/lib\n",
      "installing to /tmp/tmppd9suc9i\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/hot_dog_train.py -> /tmp/tmppd9suc9i\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmppd9suc9i/tfx_user_code_Trainer-0.0+ee43407f36cb715c678d56e4a3c2cceb6a9f5cc3725870c85bcd6b339c4857e4-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmppd9suc9i/tfx_user_code_Trainer-0.0+ee43407f36cb715c678d56e4a3c2cceb6a9f5cc3725870c85bcd6b339c4857e4.dist-info/WHEEL\n",
      "creating '/tmp/tmpr79vi12f/tfx_user_code_Trainer-0.0+ee43407f36cb715c678d56e4a3c2cceb6a9f5cc3725870c85bcd6b339c4857e4-py3-none-any.whl' and adding '/tmp/tmppd9suc9i' to it\n",
      "adding 'hot_dog_train.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+ee43407f36cb715c678d56e4a3c2cceb6a9f5cc3725870c85bcd6b339c4857e4.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+ee43407f36cb715c678d56e4a3c2cceb6a9f5cc3725870c85bcd6b339c4857e4.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+ee43407f36cb715c678d56e4a3c2cceb6a9f5cc3725870c85bcd6b339c4857e4.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+ee43407f36cb715c678d56e4a3c2cceb6a9f5cc3725870c85bcd6b339c4857e4.dist-info/RECORD'\n",
      "removing /tmp/tmppd9suc9i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TFX pipeline orchestrated with Kubeflow Pipelines\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "from tfx import v1 as tfx\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '.json'\n",
    "\n",
    "def _create_pipeline(\n",
    "    pipeline_name, pipeline_root, data_root, transform_module_file, \n",
    "    trainer_module_file, serving_model_dir\n",
    "):\n",
    "    \"\"\"Create a TFX Pipeline\"\"\"\n",
    "    example_gen = tfx.components.ImportExampleGen(input_base=data_root)\n",
    "\n",
    "    statistics_gen = tfx.components.StatisticsGen(\n",
    "        examples=example_gen.outputs['examples']\n",
    "    )\n",
    "    \n",
    "    schema_gen = tfx.components.SchemaGen(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        infer_feature_shape=True\n",
    "    )\n",
    "    \n",
    "    example_validator = tfx.components.ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema']\n",
    "    )\n",
    "    \n",
    "    transform = tfx.components.Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        module_file=transform_module_file\n",
    "    )\n",
    "  \n",
    "    # Uses user-provided Python function that implements a model.\n",
    "    trainer = tfx.components.Trainer(\n",
    "        #custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "        module_file=trainer_module_file,\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=25),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=12)\n",
    "    )\n",
    "\n",
    "    model_resolver = tfx.dsl.Resolver(\n",
    "        strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "        model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "        model_blessing=tfx.dsl.Channel(\n",
    "            type=tfx.types.standard_artifacts.ModelBlessing\n",
    "        )\n",
    "    ).with_id('latest_blessed_model_resolver')\n",
    "\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='label')],\n",
    "        slicing_specs=[tfma.SlicingSpec()],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(\n",
    "                    class_name='CategoricalAccuracy',\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={'value': 0.55}),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={'value': -1e-3}\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "    evaluator = tfx.components.Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=eval_config\n",
    "    )  \n",
    "    pusher = tfx.components.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=evaluator.outputs['blessing'],\n",
    "        push_destination=tfx.proto.PushDestination(\n",
    "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[\n",
    "            example_gen,\n",
    "            statistics_gen,\n",
    "            schema_gen,\n",
    "            example_validator,\n",
    "            transform,\n",
    "            trainer,\n",
    "            model_resolver,\n",
    "            evaluator,\n",
    "            pusher,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE\n",
    ")\n",
    "# Following function will write the pipeline definition to PIPELINE_DEFINITION_FILE.\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        transform_module_file=os.path.join(MODULE_ROOT, HOT_DOG_TRANSFORM),\n",
    "        trainer_module_file=os.path.join(MODULE_ROOT, HOT_DOG_TRAIN),\n",
    "        serving_model_dir=SERVING_MODEL_DIR,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Vertex AI Pipeline\n",
    "The pipeline definition file generated in the previous stpe can be submitted using kfp client to start a run of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tI71jlEvWMV7"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    template_path=PIPELINE_DEFINITION_FILE, display_name=PIPELINE_NAME\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False, \n",
    "    service_account='hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3k9f5IVQXcQ"
   },
   "source": [
    "Once the pipeline has been successfully submitted, we can go to __Vertex AI > Pipelines__ to view the progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Continuous Training\n",
    "Like in the Airflow example, we could also set up Cloud Scheduler so the pipeline [executes on a schedule](https://cloud.google.com/vertex-ai/docs/pipelines/schedule-cloud-scheduler). However, in this continuous training example we will set up the pipeline to rerun whenever a new file is uploaded to a storage bucket of interest. This is an example of an [Cloud Functions event-driven function](https://cloud.google.com/functions/docs/writing/write-event-driven-functions#background-functions).\n",
    "\n",
    "We need to transfer our pipeline definition file (created in step 5) to a storage bucket so we can continue to access it once this notebook instance is no longer available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://hot-dog-vertex-pipelines.json [Content-Type=application/json]...\n",
      "/ [1 files][ 18.7 KiB/ 18.7 KiB]                                                \n",
      "Operation completed over 1 objects/18.7 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Paths for pipeline definition file.\n",
    "DEFINITION_ROOT = 'gs://{}/pipeline_definition/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "!gsutil cp {PIPELINE_DEFINITION_FILE} {DEFINITION_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to files to successfully initialize a Cloud Functions: [`main.py` and `requirements.txt`](https://cloud.google.com/functions/docs/writing#directory-structure). The two files are put into a separate folder.\n",
    "\n",
    "The `main.py` file contains the code to run when the Cloud Functions is activated (i.e. a new file is put into a bucket). In our case we want to rerun the pipeline so we are wrapping the code to start the pipeline into a function (`pipeline_run`) and then wrapping that function into an entry point (`trigger_pipeline`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('cloud_function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD_FUNCTION_MAIN = 'cloud_function/main.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cloud_function/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CLOUD_FUNCTION_MAIN}\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = 'gs://hot-dog-pipeline-gcs/pipeline_definition/hot-dog-vertex-pipelines/hot-dog-vertex-pipelines.json'\n",
    "GOOGLE_CLOUD_PROJECT = 'hot-dog-pipeline'\n",
    "GOOGLE_CLOUD_REGION = 'us-central1'\n",
    "PIPELINE_NAME = 'hot-dog-vertex-pipelines'\n",
    "\n",
    "def pipeline_run():\n",
    "    aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "    job = pipeline_jobs.PipelineJob(\n",
    "        template_path=PIPELINE_DEFINITION_FILE, display_name=PIPELINE_NAME\n",
    "    )\n",
    "\n",
    "    job.run(\n",
    "        sync=False, \n",
    "        service_account='hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com'\n",
    "    )\n",
    "\n",
    "\n",
    "def trigger_pipeline(event, context):\n",
    "    \"\"\"Entry point for Cloud Function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    event : dict\n",
    "        Event payload\n",
    "    context : google.cloud.functions.Context\n",
    "         Metadata for the event.\n",
    "    \"\"\"\n",
    "    pipeline_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requirements.txt` folder contains the libraries needed for `main.py` to run. In our case, we only need the `google-cloud-aiplatform` library to run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD_FUNCTION_REQUIREMENTS = 'cloud_function/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cloud_function/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CLOUD_FUNCTION_REQUIREMENTS}\n",
    "google-cloud-aiplatform>=1.17.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a Cloud Functions function for our continuous training. The function will watch the storage bucket of interest (i.e. gs://hot-dog-pipeline-gcs-data). If any new files are observed (i.e. --trigger-event=google.storage.object.finalize), the pipeline will start (i.e. `trigger_pipeline` in `main.py`) and the model will be retrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying function (may take a while - up to 2 minutes)...⠹                    \n",
      "For Cloud Build Logs, visit: https://console.cloud.google.com/cloud-build/builds;region=us-central1/6d584940-ec55-4133-8a7f-2fad2af7c97f?project=147591496135\n",
      "Deploying function (may take a while - up to 2 minutes)...done.                \n",
      "availableMemoryMb: 256\n",
      "buildId: 6d584940-ec55-4133-8a7f-2fad2af7c97f\n",
      "buildName: projects/147591496135/locations/us-central1/builds/6d584940-ec55-4133-8a7f-2fad2af7c97f\n",
      "dockerRegistry: CONTAINER_REGISTRY\n",
      "entryPoint: trigger_pipeline\n",
      "eventTrigger:\n",
      "  eventType: google.storage.object.finalize\n",
      "  failurePolicy: {}\n",
      "  resource: projects/_/buckets/hot-dog-pipeline-gcs-data\n",
      "  service: storage.googleapis.com\n",
      "ingressSettings: ALLOW_ALL\n",
      "labels:\n",
      "  deployment-tool: cli-gcloud\n",
      "name: projects/hot-dog-pipeline/locations/us-central1/functions/pipeline_run\n",
      "runtime: python37\n",
      "serviceAccountEmail: hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com\n",
      "sourceUploadUrl: https://storage.googleapis.com/uploads-875507164094.us-central1.cloudfunctions.appspot.com/fbcabf3f-2b7c-4584-b39b-1c0742de1263.zip\n",
      "status: ACTIVE\n",
      "timeout: 400s\n",
      "updateTime: '2022-09-13T09:45:33.218Z'\n",
      "versionId: '2'\n"
     ]
    }
   ],
   "source": [
    "  !gcloud functions deploy pipeline_run \\\n",
    "  --region=us-central1 \\\n",
    "  --entry-point=trigger_pipeline \\\n",
    "  --runtime=python37 \\\n",
    "  --service-account=hot-dog-pipeline-service@hot-dog-pipeline.iam.gserviceaccount.com \\\n",
    "  --source=cloud_function \\\n",
    "  --timeout=400 \\\n",
    "  --trigger-resource=gs://hot-dog-pipeline-gcs-data\\\n",
    "  --trigger-event=google.storage.object.finalize"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pknVo1kM2wI2"
   ],
   "name": "Simple TFX Pipeline for Vertex Pipelines",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9f874cf1e3068453911091b137ff738e302d04e3712b6fc6fb69abd27ded30e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
